{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set CUDA device order and visible devices\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7,8,9\"\n",
    "\n",
    "# Set the device\n",
    "device = '/cpu:0'\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the second GPU\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            tf.config.experimental.set_visible_devices(gpus[9], 'GPU')\n",
    "            device = '/gpu:9'\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"device\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load HDF5 data\n",
    "# h5_filename = '/home/da886/ElectronCountingProject/Small Building Blocks/Data Generated/150K_3electronsOnly.h5'#### i used this for training and saved it under weekend steps weights\n",
    "# with h5py.File(h5_filename, 'r') as f:\n",
    "#     images = np.array(f['images'][:100000])\n",
    "#     centers = np.array(f['centers_training'][:100000])\n",
    "h5_filename = '/home/da886/ElectronCountingProject/Small Building Blocks/Data Generated/500_3electronsOnlyforAnalysis.h5'\n",
    "with h5py.File(h5_filename, 'r') as f:\n",
    "    images = np.array(f['images'])\n",
    "    centers = np.array(f['centers_training'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0, len(images))\n",
    "# a =2\n",
    "image = images[a]\n",
    "centerss = centers[a]\n",
    "\n",
    "# Plot the image with valid centers\n",
    "plt.imshow(image, )\n",
    "valid_centers = centerss[centerss[:, 0] == 1]\n",
    "for center in valid_centers:\n",
    "    plt.scatter(center[1], center[2], c='red', marker='x')  # Note: center[1] is x and center[2] is y\n",
    "plt.title('Image with Valid Centers Marked')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to sort centers from top-left to bottom-right\n",
    "def sort_centers(centers):\n",
    "    # Sort by y first, then by x\n",
    "    return centers[np.lexsort((centers[:, 0], centers[:, 1]))]\n",
    "\n",
    "# Sort the centers for each image\n",
    "sorted_centers = np.array([sort_centers(image_centers[:, 1:]) for image_centers in centers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalized_images = (images+1e-9)/9.26\n",
    "normalized_images = (images)/np.max(images)\n",
    "normalized_centers = sorted_centers /  64\n",
    "normalized_midpoints =normalized_centers\n",
    "normalized_midpoints = tf.expand_dims(normalized_midpoints,axis =1)\n",
    "normalized_midpoints_np = normalized_midpoints.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(normalized_images), np.min(normalized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(normalized_centers), np.min(normalized_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the random split\n",
    "train_images, val_images, train_midpoints, val_midpoints = train_test_split(\n",
    "    normalized_images, normalized_midpoints_np, train_size=0.8, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape,val_images.shape,train_midpoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2000,reshuffle_each_iteration= True).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=2000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_image, sample_midpoints = next(iter(train_dataset))\n",
    "\n",
    "\n",
    "# Convert TensorFlow tensors to NumPy arrays (if they are tensors)\n",
    "sample_image = np.array(sample_image[2])\n",
    "sample_midpoints = np.array(sample_midpoints[2])\n",
    "\n",
    "# # Transpose the image and correctly swap the midpoints\n",
    "\n",
    "transposed_image = sample_image.T\n",
    "transposed_midpoints_corrected = sample_midpoints[:,:, [1, 0]]  # Correctly swap x and y coordinates\n",
    "\n",
    "# Plot the transposed image with corrected midpoints\n",
    "plt.imshow(transposed_image, cmap='gray')\n",
    "plt.scatter(transposed_midpoints_corrected[:,:, 0] * 64, transposed_midpoints_corrected[:,:, 1] * 64, c='red', marker='o')\n",
    "plt.title('Transposed Image with Corrected Midpoints')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_midpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "input_shape = (64, 64, 1)\n",
    "num_classes = 3\n",
    "num_coordinates = 2\n",
    "\n",
    "\n",
    "x_input = layers.Input(shape=input_shape)\n",
    "#Layer 1\n",
    "x = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')(x_input)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "#Layer 2\n",
    "x = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Dropout(0.2)(x)\n",
    "#Layer 3\n",
    "x = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Dropout(0.2)(x)\n",
    "#Layer 4\n",
    "x = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "#Layer 5\n",
    "x = layers.Conv2D(256, kernel_size=5, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x) \n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "# Bounding box output\n",
    "x_midpoints = layers.Dense(num_classes * num_coordinates, activation='sigmoid', name='x_midpoints')(x)\n",
    "x_midpoints_reshape = layers.Reshape((-1, num_classes, num_coordinates), name='x_midpoints_reshape')(x_midpoints)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Model(x_input, x_midpoints_reshape)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = model.predict(train_images)\n",
    "val_outputs = model.predict(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class DynamicExponentCallback(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, initial_exponent=3, increment=0.5, update_frequency=10):\n",
    "#         super().__init__()\n",
    "#         self.exponent = initial_exponent\n",
    "#         self.increment = increment\n",
    "#         self.update_frequency = update_frequency\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if (epoch + 1) % self.update_frequency == 0:\n",
    "#             self.exponent += self.increment\n",
    "#             print(f\"\\nEpoch {epoch + 1}: Increasing exponent to {self.exponent}\")\n",
    "#             self.model.loss = self.custom_loss(self.exponent)\n",
    "\n",
    "#     def custom_loss(self, exponent):\n",
    "#         def loss(y_true, y_pred):\n",
    "#             diff = tf.abs(y_true - y_pred)\n",
    "#             powered_diff = tf.pow(diff, exponent)\n",
    "#             return tf.reduce_mean(powered_diff)\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_exponent_callback = DynamicExponentCallback(initial_exponent=3, increment=1, update_frequency=10)\n",
    "loss=dynamic_exponent_callback.custom_loss(3)\n",
    "tl =loss(train_outputs,train_midpoints)\n",
    "vl = loss(val_outputs,val_midpoints)\n",
    "\n",
    "tl,vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, callbacks\n",
    "# Custom callback to save the model every 10 epochs\n",
    "class CustomModelCheckpoint(callbacks.Callback):\n",
    "    def __init__(self, save_freq, save_path):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            self.model.save(self.save_path.format(epoch=epoch + 1))\n",
    "            print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "save_freq = 200  # Save every 10 epochs\n",
    "save_path = \"/home/da886/ElectronCountingProject/weights for custom loss/customlossmodel_epoch_{epoch:02d}.h5\"\n",
    "checkpoint_callback = CustomModelCheckpoint(save_freq=save_freq, save_path=save_path)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',       # Monitor the validation loss\n",
    "    factor=0.5,               # Factor by which the learning rate will be reduced\n",
    "    patience=5,               # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,                # Verbosity mode, 1: output logs\n",
    "    mode='min',               # Min mode, as we want to reduce the LR when the monitored quantity stops decreasing\n",
    "    min_lr=5e-14               # Lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile your model initially with the initial exponent\n",
    "\n",
    "dynamic_exponent_callback = DynamicExponentCallback(initial_exponent=3, increment=1, update_frequency=10)\n",
    "model.compile(optimizer=optimizer, loss=dynamic_exponent_callback.custom_loss(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_dataset, epochs=1700, validation_data=val_dataset,callbacks=[checkpoint_callback,lr_scheduler])\n",
    "history = model.fit(train_dataset, epochs=60, validation_data=val_dataset,callbacks=[dynamic_exponent_callback,lr_scheduler])\n",
    "# history = model_custom.fit(train_dataset, epochs=150, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Print the available keys in the history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Extract the losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# If there are specific losses for x_prob and x_midpoints, extract them\n",
    "train_loss_x_prob = history.history.get('x_prob_reshape_loss', train_loss)\n",
    "val_loss_x_prob = history.history.get('val_x_prob_reshape_loss', val_loss)\n",
    "train_loss_x_midpoints = history.history.get('x_midpoints_reshape_loss', train_loss)\n",
    "val_loss_x_midpoints = history.history.get('val_x_midpoints_reshape_loss', val_loss)\n",
    "\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting x_prob_reshape loss\n",
    "plt.subplot(1, 2,1)\n",
    "plt.plot(train_loss_x_prob, label='Train Loss ')\n",
    "plt.plot(val_loss_x_prob, label='Validation Loss ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss ')\n",
    "plt.legend()\n",
    "\n",
    "# # Plotting x_midpoints_reshape loss\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(train_loss_x_midpoints, label='Train Loss x_midpoints_reshape')\n",
    "# plt.plot(val_loss_x_midpoints, label='Validation Loss x_midpoints_reshape')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss for x_midpoints_reshape')\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u =tf.keras.models.load_model(\"/home/da886/ElectronCountingProject/weekend steps custom loss weights/customlossmodel.keras\",safe_mode=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_midpoints))\n",
    "\n",
    "train_dataset = train_dataset.batch(200)\n",
    "inputs,targets = next(iter(train_dataset))\n",
    "output =model.predict(inputs)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_midpoints))\n",
    "# val_dataset = val_dataset.batch(400)\n",
    "# inputs,targets = next(iter(val_dataset))\n",
    "# output =model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.random.randint(0,len(output))\n",
    "output[h]*64,targets[h]*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(y_true, y_pred, exponent=10):\n",
    "    diff = tf.abs(y_true - y_pred)\n",
    "    powered_diff = tf.pow(diff, exponent)\n",
    "    return tf.reduce_mean(powered_diff)\n",
    "\n",
    "# # r = np.random.randint(0,100)\n",
    "# tensor1 = tf.constant(targets, dtype=tf.float64)\n",
    "\n",
    "# tensor2 = tf.constant(output, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# tensor2 = tf.cast(tensor2, tf.float64)\n",
    "\n",
    "\n",
    "# mse_loss_fn = custom_mse(tensor1, tensor2)\n",
    "# mse_loss = mse_loss_fn(tensor1, tensor2)\n",
    "mse_loss_fn = custom_loss(targets, output, exponent=10)\n",
    "\n",
    "print(\"MSE Loss:\", mse_loss_fn.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_midpoints(image, midpoints):\n",
    "    \"\"\"\n",
    "    Visualizes midpoints on an image without using a probability vector.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "    midpoints_np = midpoints\n",
    "\n",
    "    # Denormalize image if necessary (adjust based on your normalization method)\n",
    "    denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(denormalized_image, cmap='gray')\n",
    "    plt.title(\"Predicted Midpoint Visualization\")\n",
    "\n",
    "    # Plot midpoints directly\n",
    "    for i, (x, y) in enumerate(midpoints_np):\n",
    "        plt.scatter(x, y, color='red', s=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualize_midpoints2(image, midpoints):\n",
    "    \"\"\"\n",
    "    Visualizes ground truth midpoints on an image without using a probability vector.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A 3D tensor representing the image.\n",
    "    - midpoints: A 2D tensor representing the midpoint coordinates (x, y).\n",
    "\n",
    "    Returns:\n",
    "    None (displays the image with midpoints).\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays for easier handling\n",
    "    image_np = image\n",
    "    midpoints_np = midpoints\n",
    "\n",
    "    # Denormalize image if necessary (adjust based on your normalization method)\n",
    "    denormalized_image = image_np  # Modify if normalization was applied during training\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(denormalized_image, cmap='gray')\n",
    "    plt.title(\"Ground Truth Midpoint Visualization\")\n",
    "\n",
    "    # Plot midpoints directly\n",
    "    for i, (x, y) in enumerate(midpoints_np):\n",
    "        plt.scatter(x, y, color='red', s=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with random data\n",
    "t = np.random.randint(0, 200)\n",
    "# t=5\n",
    "\n",
    "visualize_midpoints(tf.convert_to_tensor(inputs[t]), tf.convert_to_tensor(output[t,0,:,:])*64)\n",
    "visualize_midpoints2(tf.convert_to_tensor(inputs[t]), tf.convert_to_tensor(targets[t,0,:,:])*64)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
